{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 在原版paper的deepfm算法做了一点改进，即fm_component和deep component输出的都不只是一个数了，而是一个响亮\n",
    "# 大量参照https://github.com/ChenglongChen/tensorflow-DeepFM\n",
    "# 他对deep fm进行了一点点优化，虽然数据描述不太明白，但是其数据结构设计得确实好，很精简也非常适合这个算法\n",
    "# deep fm只能处理one-hot类型的，无法直接处理multi-hot特征\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat_indices_arr\n",
      "[[5. 1. 9.]\n",
      " [9. 7. 0.]\n",
      " [8. 7. 3.]\n",
      " [7. 9. 1.]\n",
      " [1. 0. 9.]\n",
      " [9. 5. 3.]\n",
      " [9. 5. 0.]\n",
      " [3. 5. 1.]\n",
      " [9. 8. 2.]]\n",
      "\n",
      "feat_vals_arr\n",
      "[[1.         1.         0.7929278 ]\n",
      " [1.         1.         0.89248794]\n",
      " [1.         1.         0.5548318 ]\n",
      " [1.         1.         0.2731279 ]\n",
      " [1.         1.         0.46400025]\n",
      " [1.         1.         0.6120409 ]\n",
      " [1.         1.         0.51371866]\n",
      " [1.         1.         0.6585211 ]\n",
      " [1.         1.         0.74477696]]\n",
      "\n",
      "emb vectors\n",
      "tf.Tensor(\n",
      "[[[ 0.01684952 -0.04023438  0.03148881 -0.00977103]\n",
      "  [-0.00602516 -0.00801171 -0.02596194 -0.04596521]\n",
      "  [ 0.02065054  0.02518673 -0.01142569 -0.01360821]]\n",
      "\n",
      " [[ 0.0260434   0.03176421 -0.01440949 -0.01716198]\n",
      "  [-0.02003713 -0.02100695  0.02142284  0.01265253]\n",
      "  [ 0.03413467  0.02389419 -0.00540351  0.02527387]]\n",
      "\n",
      " [[ 0.01591739  0.00670235  0.04473504  0.00274844]\n",
      "  [-0.02003713 -0.02100695  0.02142284  0.01265253]\n",
      "  [ 0.01365053 -0.00405248 -0.01630789 -0.00757939]]\n",
      "\n",
      " [[-0.02003713 -0.02100695  0.02142284  0.01265253]\n",
      "  [ 0.0260434   0.03176421 -0.01440949 -0.01716198]\n",
      "  [-0.00164564 -0.00218822 -0.00709093 -0.01255438]]\n",
      "\n",
      " [[-0.00602516 -0.00801171 -0.02596194 -0.04596521]\n",
      "  [ 0.03824664  0.02677256 -0.00605444  0.02831844]\n",
      "  [ 0.01208415  0.0147386  -0.00668601 -0.00796316]]], shape=(5, 3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,feat_dim,field_num,emb_dim,*args,**kwargs):\n",
    "        \"\"\"\n",
    "\n",
    "        :param feat_dim: int 每个样本的特征维度，假如每个样本可以被表征为[0,0,0,1,0,1,2]，那feat_dim就应该是7\n",
    "        :param field_num: int 每个样本的特征的field_num，可以理解为有多少种特征，例如一个样本有性别和年龄两类特征(特征向量可能为[0,1,12])，那field_num就是2\n",
    "        :param emb_dim: int 对于每个field的嵌入向量维度\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "        super(EmbeddingLayer,self).__init__(*args,**kwargs)\n",
    "        self.feat_dim=feat_dim\n",
    "        self.field_num=field_num\n",
    "        self.emb_dim=emb_dim\n",
    "        self.emb_layer=tf.keras.layers.Embedding(input_dim=feat_dim,output_dim=emb_dim)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        :param inputs: (feat_indices_batch, feat_value_batch)，分为两部分，你可以把他看做一个手动构造的稀疏矩阵\n",
    "            例如，如果feat_indices_batch的数据为[[1,5,9],[2,7,8]]；feat_value_batch的数据为[[1,1,2.3],[1,1,0.98]]\n",
    "            假设第一个样本的特征向量是x，那么x[1]=1, x[5]=1, x[9]=2.3，其余位置取值均为0。\n",
    "            这样构造是因为每个样本都有field_num个field，每个field的取值只有一种（one-hot或者连续值）\n",
    "            也就是说每个样本都有field_num个不为0的特征维度。而deep fm算法的嵌入方法是对每一个field嵌入，不管是不是连续值都要嵌入，然后再乘以特征取值\n",
    "            例如x[9]=2.3，那就要从emb_table里找到第9个emb_vector，然后乘以2.3\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # feat_indices_batch: [batch_size, field_num]\n",
    "        # feat_value_batch: [batch_size, field_num]\n",
    "        feat_indices_batch,feat_value_batch=inputs\n",
    "        # 两者形状要相同，并且二者的第二个轴取值维度都是field_num个\n",
    "        assert feat_indices_batch.shape==feat_value_batch.shape\n",
    "        assert feat_indices_batch.shape[1:]==[self.field_num]\n",
    "\n",
    "        emb_vectors=self.emb_layer(feat_indices_batch) # [batch_size, field_num, emb_dim]\n",
    "        feat_value_batch = tf.expand_dims(feat_value_batch,axis=-1) # [batch_size, field_num, 1]\n",
    "\n",
    "        # broadcast性质 feat_value_batch会被看做[batch_size, field_num, emb_dim]\n",
    "        emb_vectors = tf.multiply(emb_vectors,feat_value_batch) # [batch_size, field_num, emb_dim]\n",
    "        return emb_vectors\n",
    "\n",
    "\n",
    "feat_indices_arr=[np.random.choice(range(10),size=[1,3],replace=False) for _ in range(9)]\n",
    "feat_indices_arr=np.concatenate(feat_indices_arr,axis=0).astype(np.float32)\n",
    "print(\"feat_indices_arr\")\n",
    "print(feat_indices_arr) #[10,3]\n",
    "\n",
    "feat_vals_arr=np.concatenate((np.ones(shape=[9,2]),\n",
    "                              np.random.random(size=[9,1])),axis=1).astype(np.float32)\n",
    "print(\"\\nfeat_vals_arr\")\n",
    "print(feat_vals_arr) # [10,3]\n",
    "\n",
    "input_ds=tf.data.Dataset.from_tensor_slices((feat_indices_arr,feat_vals_arr))\n",
    "batched_ds=input_ds.batch(5)\n",
    "iterator=iter(batched_ds)\n",
    "input_batch=next(iterator)\n",
    "\n",
    "\n",
    "print(\"\\nemb vectors\")\n",
    "emb_layer=EmbeddingLayer(feat_dim=10,field_num=3,emb_dim=4)\n",
    "emb_vectors=emb_layer(input_batch)\n",
    "print(emb_vectors)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "emb vectors\n",
      "tf.Tensor(\n",
      "[[[-0.03016762 -0.02599541 -0.00467795 -0.04329629]\n",
      "  [ 0.00803028 -0.03629998  0.00394214  0.03103927]\n",
      "  [-0.01044718 -0.00198661  0.0081759   0.02610639]\n",
      "  ...\n",
      "  [-0.00034495 -0.03942304 -0.00878468 -0.00130062]\n",
      "  [ 0.00112212 -0.00829253 -0.00185618  0.00145497]\n",
      "  [-0.01404082  0.03171968  0.01547828  0.02390836]]\n",
      "\n",
      " [[-0.00799986 -0.01253077 -0.00314962  0.01862652]\n",
      "  [ 0.02455476 -0.02964035 -0.02909072  0.0354126 ]\n",
      "  [ 0.04982335  0.00999033 -0.02765146 -0.01071627]\n",
      "  ...\n",
      "  [ 0.03765546 -0.01602382  0.03813593  0.00277154]\n",
      "  [ 0.00711776  0.02643858 -0.02162612 -0.00900532]\n",
      "  [ 0.00067139 -0.00327853  0.00467062  0.00072237]]\n",
      "\n",
      " [[-0.03388724  0.02102203 -0.00727738 -0.02720662]\n",
      "  [ 0.03614216 -0.00052707  0.00784385  0.03627434]\n",
      "  [-0.04883645  0.03580966 -0.04068707  0.01569316]\n",
      "  ...\n",
      "  [-0.00240174 -0.00995342  0.00579038  0.00817747]\n",
      "  [-0.00467994  0.01093904  0.00651143 -0.00203572]\n",
      "  [-0.00718298 -0.00533949  0.00218272 -0.00767821]]\n",
      "\n",
      " [[ 0.0497408   0.02773948  0.02575709 -0.02362382]\n",
      "  [ 0.00681623 -0.03260443  0.04056683  0.0114413 ]\n",
      "  [ 0.02245359 -0.02117619 -0.00517058 -0.01353858]\n",
      "  ...\n",
      "  [ 0.00163262  0.00102703  0.00348209  0.0005647 ]\n",
      "  [-0.01354045 -0.00786667  0.01446554 -0.00507652]\n",
      "  [-0.00020208 -0.00168849  0.00374033  0.00589669]]\n",
      "\n",
      " [[-0.02417603  0.00231739 -0.01443014 -0.03840559]\n",
      "  [ 0.03977824 -0.00440425  0.0293883  -0.00265633]\n",
      "  [-0.0267411  -0.02058556  0.0241897   0.03635057]\n",
      "  ...\n",
      "  [ 0.00239638 -0.03159937  0.01965804 -0.00619135]\n",
      "  [ 0.00506814  0.00102924  0.00244422  0.00019451]\n",
      "  [-0.00294968 -0.0009201   0.00192678  0.00132918]]], shape=(5, 3000, 4), dtype=float32)\n",
      "\n",
      "fm_outputs\n",
      "tf.Tensor(\n",
      "[[ 88.68658     2.2107282 -31.94742     5.164775 ]\n",
      " [-70.32456    19.99273    90.15897   -19.370964 ]\n",
      " [ 23.32513   -41.59575   -48.615273   29.518568 ]\n",
      " [ 44.517437  -22.187275   16.63677    13.0285225]\n",
      " [ -7.2089276  48.760357  -45.967495  -13.2091255]], shape=(5, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class FMComponent(tf.keras.layers.Layer):\n",
    "    def __init__(self,feat_dim,field_num,emb_dim,*args,**kwargs):\n",
    "        \"\"\"\n",
    "        同EmbeddingLayer\n",
    "        :param feat_dim:\n",
    "        :param field_num:\n",
    "        :param emb_dim:\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "        super(FMComponent,self).__init__(*args,**kwargs)\n",
    "        self.feat_dim=feat_dim\n",
    "        self.field_num=field_num\n",
    "        self.emb_dim=emb_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w=tf.Variable(initial_value=tf.random.truncated_normal(shape=[self.feat_dim, self.emb_dim]))\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\"\n",
    "\n",
    "        :param inputs: (raw_input_batch,emb_vectors)\n",
    "            其中raw_input_batch是feat_indices_batch,feat_value_batch 其实就是EmbeddingLayer的输入 用于计算一阶term\n",
    "            emb_vectors是后者是emb_layer的输出\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        raw_input_batch,emb_vectors=inputs # emb_vectors: [batch_size, field_num, emb_dim]\n",
    "        # feat_indices_batch: [batch_size, field_num]\n",
    "        # feat_value_batch: [batch_size, field_num]\n",
    "        feat_indices_batch,feat_value_batch=raw_input_batch\n",
    "\n",
    "        # first order term\n",
    "        # 使用feat_indices找到embedding_lookup快速找到field_num个权重然后做相乘\n",
    "        # 例如，如果一个样本x，他在特征维度1、3、5上有取值，那么他的feat_indices=[1,3,5]。那只需要从self.w找到第1、3、5个数就可以了\n",
    "        # 这样的计算方法更加快速\n",
    "        # 改进1 一阶输出也是一个向量而非一个标量，这就要求self.w的shape为[self.feat_dim, self.emb_dim]\n",
    "        # 原本deepFM之中女\n",
    "        # 一阶做的事情实际上就是，假如一个样本有三个field上的取值为[1,1,1.3]，特征id分别是1，3，5，那么一阶结果就是w1*1+w3*1+w5*1.3\n",
    "        # 拓展版本就是将w1换成了一个长度为emb_size的向量\n",
    "        weights=tf.nn.embedding_lookup(params=self.w,ids=tf.cast(feat_indices_batch,tf.int32)) # [batch_size, field_num, self.emb_dim]\n",
    "        # 需要对feat_value_batch扩充一下，不然无法进行broadcast\n",
    "        first_order_term = tf.multiply(tf.expand_dims(feat_value_batch,axis=2),weights) # [batch_size, field_num, emb_dim]\n",
    "        first_order_term = tf.reduce_sum(first_order_term,axis=1) # [batch_size, emb_dim]\n",
    "\n",
    "        # second order term\n",
    "        # 下面这个是fm算法的优化算法 和平方减去平方和\n",
    "        sum_square=tf.square(tf.reduce_sum(emb_vectors,axis=1)) # [batch_size, emb_dim]\n",
    "        square_sum=tf.reduce_sum(tf.square(emb_vectors),axis=1) # [batch_size, emb_dim]\n",
    "\n",
    "        second_order_term=1/2*tf.subtract(sum_square,square_sum) # [batch_size, emb_dim]\n",
    "        y_fm=first_order_term+second_order_term\n",
    "        return y_fm\n",
    "\n",
    "\n",
    "# 突出一个问题，如果num_fields过多，会导致fm_output的数值膨胀\n",
    "num_fields=3000\n",
    "num_features=30000\n",
    "feat_indices_arr=[np.random.choice(range(1,num_features),size=[1,num_fields],replace=False) for _ in range(9)]\n",
    "feat_indices_arr=np.concatenate(feat_indices_arr,axis=0).astype(np.float32)\n",
    "feat_vals_arr=np.concatenate((np.ones(shape=[9,2000]),\n",
    "                              np.random.random(size=[9,1000])),axis=1).astype(np.float32)\n",
    "\n",
    "input_ds=tf.data.Dataset.from_tensor_slices((feat_indices_arr,feat_vals_arr))\n",
    "batched_ds=input_ds.batch(5)\n",
    "iterator=iter(batched_ds)\n",
    "input_batch=next(iterator)\n",
    "\n",
    "\n",
    "print(\"\\nemb vectors\")\n",
    "emb_layer=EmbeddingLayer(feat_dim=num_features,field_num=num_fields,emb_dim=4)\n",
    "emb_vectors=emb_layer(input_batch)\n",
    "print(emb_vectors)\n",
    "\n",
    "fm_component=FMComponent(feat_dim=num_features,field_num=num_fields,emb_dim=4)\n",
    "fm_inputs=(input_batch,emb_vectors)\n",
    "fm_outputs=fm_component(fm_inputs)\n",
    "print(\"\\nfm_outputs\")\n",
    "print(fm_outputs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class DeepComponent(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    深层网络，没啥好说的\n",
    "    \"\"\"\n",
    "    def __init__(self,deep_units_list,*args,**kwargs):\n",
    "        super(DeepComponent,self).__init__(*args,**kwargs)\n",
    "        self.deep_layers=list()\n",
    "        for deep_units in deep_units_list:\n",
    "            self.deep_layers.append(tf.keras.layers.Dense(units=deep_units,activation=tf.nn.relu))\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        for deep_layer in self.deep_layers:\n",
    "            inputs=deep_layer(inputs)\n",
    "        return inputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(5, 2), dtype=float32, numpy=\narray([[-0.26829186, -0.108273  ],\n       [-1.1350449 ,  2.1167572 ],\n       [-1.3979748 ,  1.5567256 ],\n       [-0.7603167 ,  1.079642  ],\n       [-0.16347432, -1.9003195 ]], dtype=float32)>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class DeepFM(tf.keras.Model):\n",
    "    def __init__(self,feat_dim,field_num,emb_dim,deep_units_list,scoring_units=2,*args,**kwargs):\n",
    "        \"\"\"\n",
    "        同EmbeddingLayer描述\n",
    "        :param feat_dim:\n",
    "        :param field_num:\n",
    "        :param emb_dim:\n",
    "        :param deep_units_list:\n",
    "        :param scoring_units: int 输出的类别数目，两类问题就是2，三类问题就是3\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "        super(DeepFM,self).__init__(*args,**kwargs)\n",
    "\n",
    "        self.emb_layer=EmbeddingLayer(feat_dim=feat_dim,field_num=field_num,emb_dim=emb_dim)\n",
    "        self.fm_component=FMComponent(feat_dim=feat_dim,field_num=field_num,emb_dim=emb_dim)\n",
    "        self.deep_component=DeepComponent(deep_units_list=deep_units_list)\n",
    "        self.scoring_layer=tf.keras.layers.Dense(units=scoring_units,activation=None)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        emb_vectors=self.emb_layer(inputs)\n",
    "\n",
    "        fm_inputs=(inputs,emb_vectors)\n",
    "        y_fm=self.fm_component(fm_inputs)\n",
    "\n",
    "        deep_inputs=tf.reshape(emb_vectors,shape=[emb_vectors.shape[0],-1])\n",
    "        y_deep=self.deep_component(deep_inputs)\n",
    "        y = self.scoring_layer(tf.concat((y_fm,y_deep),axis=1))\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "feat_indices_arr=[np.random.choice(range(10),size=[1,3],replace=False) for _ in range(9)]\n",
    "feat_indices_arr=np.concatenate(feat_indices_arr,axis=0).astype(np.float32)\n",
    "feat_vals_arr=np.concatenate((np.ones(shape=[9,2]),\n",
    "                              np.random.random(size=[9,1])),axis=1).astype(np.float32)\n",
    "\n",
    "input_ds=tf.data.Dataset.from_tensor_slices((feat_indices_arr,feat_vals_arr))\n",
    "batched_ds=input_ds.batch(5)\n",
    "iterator=iter(batched_ds)\n",
    "input_batch=next(iterator)\n",
    "\n",
    "deep_fm_model=DeepFM(feat_dim=10,field_num=3000,emb_dim=4,deep_units_list=[10,8])\n",
    "deep_fm_model(input_batch)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.67044261 0.75686147]\n",
      " [0.14368512 0.98200156]\n",
      " [0.7414385  0.31037816]]\n",
      "tf.Tensor(\n",
      "[[0.47840872 0.52159128]\n",
      " [0.30188948 0.69811052]\n",
      " [0.60612684 0.39387316]], shape=(3, 2), dtype=float64)\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}