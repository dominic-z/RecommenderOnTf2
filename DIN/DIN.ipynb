{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_emb.shape (10, 18)\n",
      "user_behaviors_emb.shape (10, 7, 12)\n",
      "ad_emb.shape (10, 12)\n",
      "context_emb.shape (10, 18)\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,user_feat_num,user_field_num,\n",
    "                 ad_feat_num,ad_field_num,\n",
    "                 context_feat_num,context_field_num,emb_dim,*args,**kwargs):\n",
    "        super(EmbeddingLayer,self).__init__(*args,**kwargs)\n",
    "        self.user_feat_num=user_feat_num\n",
    "        self.user_field_num=user_field_num\n",
    "        self.ad_feat_num=ad_feat_num\n",
    "        self.ad_field_num=ad_field_num\n",
    "        self.context_feat_num=context_feat_num\n",
    "        self.context_field_num=context_field_num\n",
    "        self.emb_dim=emb_dim\n",
    "\n",
    "        self.user_feat_emb_layer=tf.keras.layers.Embedding(input_dim=self.user_feat_num+1,output_dim=self.emb_dim)\n",
    "        self.ad_feat_emb_layer=tf.keras.layers.Embedding(input_dim=self.ad_feat_num+1,output_dim=self.emb_dim)\n",
    "        self.context_feat_emb_layer=tf.keras.layers.Embedding(input_dim=self.context_feat_num+1,output_dim=self.emb_dim)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        # user_feat_batch: [batch_size, user_field_num]\n",
    "        # user_behaviors_batch: [batch_size, seq_len, ad_field_dim]\n",
    "        # ad_feat_batch: [batch_size, ad_field_dim]\n",
    "        # context_feat_batch: [batch_size, context_field_num]\n",
    "        user_feat_batch,user_behaviors_batch,ad_feat_batch,context_feat_batch=inputs\n",
    "\n",
    "        user_emb=self.user_feat_emb_layer(user_feat_batch) # [batch_size, user_field_num, emb_dim]\n",
    "        user_emb=tf.reshape(user_emb,shape=[-1,self.user_field_num*self.emb_dim]) # [batch_size, user_field_num*emb_dim]\n",
    "\n",
    "        user_behaviors_emb=self.ad_feat_emb_layer(user_behaviors_batch) # [batch_size, seq_len, ad_field_dim, emb_dim]\n",
    "        seq_len=user_behaviors_batch.shape[1]\n",
    "        user_behaviors_emb=tf.reshape(user_behaviors_emb,shape=[-1, seq_len,self.ad_field_num*self.emb_dim]) # [batch_size, seq_len, ad_field_dim*emb_dim]\n",
    "\n",
    "        ad_emb=self.ad_feat_emb_layer(ad_feat_batch) # [batch_size, ad_field_num, emb_dim]\n",
    "        ad_emb=tf.reshape(ad_emb,shape=[-1,self.ad_field_num*self.emb_dim]) # [batch_size, ad_field_num*emb_dim]\n",
    "\n",
    "        context_emb=self.context_feat_emb_layer(context_feat_batch) # [batch_size, context_field_num, emb_dim]\n",
    "        context_emb=tf.reshape(context_emb,shape=[-1,self.context_field_num*self.emb_dim]) # [batch_size, context_field_num*emb_dim]\n",
    "\n",
    "        return (user_emb,user_behaviors_emb,ad_emb,context_emb)\n",
    "\n",
    "user_feat_num=3\n",
    "user_field_num=3\n",
    "ad_feat_num=4\n",
    "ad_field_num=2\n",
    "context_feat_num=5\n",
    "context_field_num=3\n",
    "emb_dim=6\n",
    "batch_size=10\n",
    "seq_len=7\n",
    "\n",
    "user_feat_batch=np.random.randint(1,1+user_feat_num,size=[batch_size,user_field_num]).astype(np.int32)\n",
    "\n",
    "user_behaviors=[]\n",
    "for _ in range(batch_size):\n",
    "    seq=np.random.choice(range(1,1+ad_feat_num),size=[np.random.randint(1,1+seq_len),2]).astype(np.int32).tolist()\n",
    "    user_behaviors.append(seq)\n",
    "user_behaviors=tf.keras.preprocessing.sequence.pad_sequences(user_behaviors,maxlen=seq_len,padding=\"post\")\n",
    "\n",
    "ad_feat_batch=np.random.randint(1,1+ad_feat_num,size=[batch_size,ad_field_num]).astype(np.int32)\n",
    "context_feat_batch=np.random.randint(1,1+context_feat_num,size=[batch_size,context_field_num]).astype(np.int32)\n",
    "\n",
    "emb_layer=EmbeddingLayer(user_feat_num=user_feat_num,\n",
    "user_field_num=user_field_num,\n",
    "ad_feat_num=ad_feat_num,\n",
    "ad_field_num=ad_field_num,\n",
    "context_feat_num=context_feat_num,\n",
    "context_field_num=context_field_num,emb_dim=emb_dim)\n",
    "\n",
    "inputs=(user_feat_batch,user_behaviors,ad_feat_batch,context_feat_batch)\n",
    "user_emb,user_behaviors_emb,ad_emb,context_emb=emb_layer(inputs)\n",
    "print(\"user_emb.shape\",user_emb.shape)\n",
    "print(\"user_behaviors_emb.shape\",user_behaviors_emb.shape)\n",
    "print(\"ad_emb.shape\",ad_emb.shape)\n",
    "print(\"context_emb.shape\",context_emb.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 12)\n"
     ]
    }
   ],
   "source": [
    "class AttentionLayerCell:\n",
    "    PRelu=\"PRelu\"\n",
    "    DICE=\"DICE\"\n",
    "\n",
    "class InterestLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,units_list=None,cell_type=AttentionLayerCell.PRelu,p_relu_alpha=0.2,*args,**kwargs):\n",
    "        super(InterestLayer,self).__init__(*args,**kwargs)\n",
    "        units_list=[36] if units_list is None else units_list\n",
    "        if cell_type == AttentionLayerCell.PRelu:\n",
    "            self.dense_layers=[tf.keras.layers.Dense(units=units,activation=lambda x:tf.nn.leaky_relu(x,alpha=p_relu_alpha)) for units in units_list]\n",
    "        else:\n",
    "            raise ValueError(\"invalid AttentionLayerCell\")\n",
    "        self.scoring_layer=tf.keras.layers.Dense(units=1,activation=None)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        # user_behaviors: [batch_size, seq_len, ad_field_num]\n",
    "        # user_behaviors_emb: [batch_size, seq_len, emb_dim]\n",
    "        # ad_emb: [batch_size, emb_dim]\n",
    "        user_behaviors,user_behaviors_emb,ad_emb=inputs\n",
    "\n",
    "        # calc attention scores\n",
    "        # seq_len=user_behaviors.shape[1]\n",
    "        # emb_dim=ad_emb.shape[-1]\n",
    "        # ad_emb=tf.reshape(tf.tile(ad_emb,multiples=[1,seq_len]),shape=[-1,seq_len,emb_dim]) #[batch_size, seq_len, emb_dim]\n",
    "        ad_emb=tf.broadcast_to(tf.expand_dims(ad_emb,axis=1),shape=user_behaviors_emb.shape) #[batch_size, seq_len, emb_dim]\n",
    "        out_product=tf.multiply(user_behaviors_emb,ad_emb) #[batch_size, seq_len, emb_dim]\n",
    "\n",
    "        dense_inputs=tf.concat([user_behaviors_emb,out_product,ad_emb],axis=-1) #[batch_size, seq_len, emb_dim*3]\n",
    "        for dense_layer in self.dense_layers:\n",
    "            dense_inputs=dense_layer(dense_inputs)\n",
    "\n",
    "        # dense_inputs: [batch_size, seq_len, units]\n",
    "        attention_scores=self.scoring_layer(dense_inputs) # [batch_size, seq_len, 1]\n",
    "        interest_emb=tf.multiply(attention_scores,user_behaviors_emb) # [batch_size, seq_len, emb_dim]\n",
    "\n",
    "        # mask\n",
    "        mask=tf.expand_dims(tf.where(tf.not_equal(user_behaviors[:,:,0],0),x=1.,y=0.),axis=-1) # [batch_size, seq_len, 1]\n",
    "        interest_emb=tf.multiply(interest_emb,mask)\n",
    "        interest_emb=tf.reduce_sum(interest_emb,axis=1) # [batch_size, emb_dim]\n",
    "        return interest_emb\n",
    "\n",
    "\n",
    "inputs=(user_behaviors,user_behaviors_emb,ad_emb)\n",
    "interest_layer=InterestLayer()\n",
    "interest_emb = interest_layer(inputs)\n",
    "print(interest_emb.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class DIN(tf.keras.Model):\n",
    "    def __init__(self,user_feat_num,user_field_num,\n",
    "                 ad_feat_num,ad_field_num,\n",
    "                 context_feat_num,context_field_num,emb_dim,\n",
    "                 dense_units_list=None,dense_cell_type=AttentionLayerCell.PRelu,dense_p_relu_alpha=0.2,\n",
    "                 attention_units_list=None,attention_cell_type=AttentionLayerCell.PRelu,attention_p_relu_alpha=0.2,\n",
    "                 *args,**kwargs):\n",
    "        super(DIN,self).__init__(*args,**kwargs)\n",
    "        self.emb_layer=EmbeddingLayer(user_feat_num=user_feat_num,user_field_num=user_field_num,\n",
    "                                      ad_feat_num=ad_feat_num,ad_field_num=ad_field_num,\n",
    "                                      context_feat_num=context_feat_num,context_field_num=context_field_num,\n",
    "                                      emb_dim=emb_dim)\n",
    "        self.interest_layer=InterestLayer(units_list=attention_units_list,cell_type=attention_cell_type,\n",
    "                                          p_relu_alpha=attention_p_relu_alpha)\n",
    "        dense_units_list=[200,80] if dense_units_list is None else dense_units_list\n",
    "        if dense_cell_type == AttentionLayerCell.PRelu:\n",
    "            self.dense_layers=[tf.keras.layers.Dense(units=units,activation=lambda x:tf.nn.leaky_relu(x,alpha=dense_p_relu_alpha)) for units in dense_units_list]\n",
    "        else:\n",
    "            raise ValueError(\"invalid AttentionLayerCell\")\n",
    "        self.scoring_layer=tf.keras.layers.Dense(units=2,activation=None)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        _,user_behaviors,_,_=inputs\n",
    "        user_emb,user_behaviors_emb,ad_emb,context_emb=self.emb_layer(inputs)\n",
    "        interest_emb=self.interest_layer((user_behaviors,user_behaviors_emb,ad_emb))\n",
    "\n",
    "        dense_inputs=tf.concat([user_emb,interest_emb,context_emb],axis=1)\n",
    "        for dense_layer in self.dense_layers:\n",
    "            dense_inputs=dense_layer(dense_inputs)\n",
    "        output=self.scoring_layer(dense_inputs)\n",
    "\n",
    "        return output\n",
    "\n",
    "user_feat_num=3\n",
    "user_field_num=3\n",
    "ad_feat_num=4\n",
    "ad_field_num=2\n",
    "context_feat_num=5\n",
    "context_field_num=3\n",
    "emb_dim=6\n",
    "batch_size=10\n",
    "seq_len=7\n",
    "user_feat_batch=np.random.randint(1,1+user_feat_num,size=[batch_size,user_field_num]).astype(np.int32)\n",
    "\n",
    "user_behaviors=[]\n",
    "for _ in range(batch_size):\n",
    "    seq=np.random.choice(range(1,1+ad_feat_num),size=[np.random.randint(1,1+seq_len),2]).astype(np.int32).tolist()\n",
    "    user_behaviors.append(seq)\n",
    "user_behaviors=tf.keras.preprocessing.sequence.pad_sequences(user_behaviors,maxlen=seq_len,padding=\"post\")\n",
    "\n",
    "ad_feat_batch=np.random.randint(1,1+ad_feat_num,size=[batch_size,ad_field_num]).astype(np.int32)\n",
    "context_feat_batch=np.random.randint(1,1+context_feat_num,size=[batch_size,context_field_num]).astype(np.int32)\n",
    "\n",
    "din=DIN(user_feat_num=user_feat_num,\n",
    "user_field_num=user_field_num,\n",
    "ad_feat_num=ad_feat_num,\n",
    "ad_field_num=ad_field_num,\n",
    "context_feat_num=context_feat_num,\n",
    "context_field_num=context_field_num,emb_dim=emb_dim)\n",
    "\n",
    "inputs=(user_feat_batch,user_behaviors,ad_feat_batch,context_feat_batch)\n",
    "output=din(inputs)\n",
    "print(output.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}