{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_emb.shape (10, 18)\n",
      "user_behaviors_emb.shape (10, 7, 12)\n",
      "ad_emb.shape (10, 12)\n",
      "context_emb.shape (10, 18)\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,user_feat_num,user_field_num,\n",
    "                 ad_feat_num,ad_field_num,\n",
    "                 context_feat_num,context_field_num,emb_dim,*args,**kwargs):\n",
    "        super(EmbeddingLayer,self).__init__(*args,**kwargs)\n",
    "        self.user_feat_num=user_feat_num\n",
    "        self.user_field_num=user_field_num\n",
    "        self.ad_feat_num=ad_feat_num\n",
    "        self.ad_field_num=ad_field_num\n",
    "        self.context_feat_num=context_feat_num\n",
    "        self.context_field_num=context_field_num\n",
    "        self.emb_dim=emb_dim\n",
    "\n",
    "        self.user_feat_emb_layer=tf.keras.layers.Embedding(input_dim=self.user_feat_num+1,output_dim=self.emb_dim)\n",
    "        self.ad_feat_emb_layer=tf.keras.layers.Embedding(input_dim=self.ad_feat_num+1,output_dim=self.emb_dim)\n",
    "        self.context_feat_emb_layer=tf.keras.layers.Embedding(input_dim=self.context_feat_num+1,output_dim=self.emb_dim)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        # user_feat_batch: [batch_size, user_field_num]\n",
    "        # user_behaviors_batch: [batch_size, seq_len, ad_field_dim]\n",
    "        # ad_feat_batch: [batch_size, ad_field_dim]\n",
    "        # context_feat_batch: [batch_size, context_field_num]\n",
    "        user_feat_batch,user_behaviors_batch,ad_feat_batch,context_feat_batch=inputs\n",
    "\n",
    "        user_emb=self.user_feat_emb_layer(user_feat_batch) # [batch_size, user_field_num, emb_dim]\n",
    "        user_emb=tf.reshape(user_emb,shape=[-1,self.user_field_num*self.emb_dim]) # [batch_size, user_field_num*emb_dim]\n",
    "\n",
    "        user_behaviors_emb=self.ad_feat_emb_layer(user_behaviors_batch) # [batch_size, seq_len, ad_field_dim, emb_dim]\n",
    "        seq_len=user_behaviors_batch.shape[1]\n",
    "        user_behaviors_emb=tf.reshape(user_behaviors_emb,shape=[-1, seq_len,self.ad_field_num*self.emb_dim]) # [batch_size, seq_len, ad_field_dim*emb_dim]\n",
    "\n",
    "        ad_emb=self.ad_feat_emb_layer(ad_feat_batch) # [batch_size, ad_field_num, emb_dim]\n",
    "        ad_emb=tf.reshape(ad_emb,shape=[-1,self.ad_field_num*self.emb_dim]) # [batch_size, ad_field_num*emb_dim]\n",
    "\n",
    "        context_emb=self.context_feat_emb_layer(context_feat_batch) # [batch_size, context_field_num, emb_dim]\n",
    "        context_emb=tf.reshape(context_emb,shape=[-1,self.context_field_num*self.emb_dim]) # [batch_size, context_field_num*emb_dim]\n",
    "\n",
    "        return (user_emb,user_behaviors_emb,ad_emb,context_emb)\n",
    "\n",
    "user_feat_num=3\n",
    "user_field_num=3\n",
    "ad_feat_num=4\n",
    "ad_field_num=2\n",
    "context_feat_num=5\n",
    "context_field_num=3\n",
    "emb_dim=6\n",
    "batch_size=10\n",
    "seq_len=7\n",
    "\n",
    "user_feat_batch=np.random.randint(1,1+user_feat_num,size=[batch_size,user_field_num]).astype(np.int32)\n",
    "\n",
    "user_behaviors=[]\n",
    "for _ in range(batch_size):\n",
    "    seq=np.random.choice(range(1,1+ad_feat_num),size=[np.random.randint(1,1+seq_len),2]).astype(np.int32).tolist()\n",
    "    user_behaviors.append(seq)\n",
    "user_behaviors=tf.keras.preprocessing.sequence.pad_sequences(user_behaviors,maxlen=seq_len,padding=\"post\")\n",
    "\n",
    "ad_feat_batch=np.random.randint(1,1+ad_feat_num,size=[batch_size,ad_field_num]).astype(np.int32)\n",
    "context_feat_batch=np.random.randint(1,1+context_feat_num,size=[batch_size,context_field_num]).astype(np.int32)\n",
    "\n",
    "emb_layer=EmbeddingLayer(user_feat_num=user_feat_num,\n",
    "user_field_num=user_field_num,\n",
    "ad_feat_num=ad_feat_num,\n",
    "ad_field_num=ad_field_num,\n",
    "context_feat_num=context_feat_num,\n",
    "context_field_num=context_field_num,emb_dim=emb_dim)\n",
    "\n",
    "inputs=(user_feat_batch,user_behaviors,ad_feat_batch,context_feat_batch)\n",
    "user_emb,user_behaviors_emb,ad_emb,context_emb=emb_layer(inputs)\n",
    "print(\"user_emb.shape\",user_emb.shape)\n",
    "print(\"user_behaviors_emb.shape\",user_behaviors_emb.shape)\n",
    "print(\"ad_emb.shape\",ad_emb.shape)\n",
    "print(\"context_emb.shape\",context_emb.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 12)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "class PRelu:\n",
    "    def __init__(self,alpha=0.2):\n",
    "        self.alpha=alpha\n",
    "\n",
    "    def __call__(self,x, *args, **kwargs):\n",
    "        return tf.nn.leaky_relu(x,alpha=self.alpha)\n",
    "\n",
    "class Dice:\n",
    "    def __init__(self,epsilon=10e-8,alpha=0.2,decay=0.99):\n",
    "        self.epsilon=epsilon\n",
    "        self.alpha=alpha\n",
    "        self.variable_mean=None\n",
    "        self.variable_var=None\n",
    "        self.ema=tf.train.ExponentialMovingAverage(decay=decay)\n",
    "\n",
    "    def __call__(self,x,training=True, *args, **kwargs):\n",
    "\n",
    "        mean=tf.reduce_mean(x,axis=0,keepdims=True) # 根据batch计算平均，在在InterestLayer中，结果为[1, seq_len, emb_dim*3]\n",
    "        var=tf.math.reduce_variance(x,axis=0,keepdims=True) # 根据batch计算平均，在InterestLayer中，结果为[1, seq_len, emb_dim*3]\n",
    "        # 因为ema只接受variable作为参数，但mean是一个Tensor，所以需要将mean转为Variable\n",
    "        # 但不可以使用直接使用tf.Variable(mean)，因为该操作会导致每次layer的call函数运行时都创建一个新的Variable，如果这样做的话，ema每次都会去追踪一个新的变量的滑动平均值\n",
    "        # 在测试过程中，tf.Variable(mean)也是一个新的变量，ema并不知道这个变量的滑动平均值，所以对他的滑动平均就会返回一个None\n",
    "        # 因此需要在函数外生成唯一的Varaible，每次对其进行assign，保证对于ema来说，输入的变量都是同一个\n",
    "        if self.variable_mean is None:\n",
    "            print(\"initialize variable_mean and variable_var\")\n",
    "            self.variable_mean=tf.Variable(tf.zeros_like(mean))\n",
    "            self.variable_var=tf.Variable(tf.zeros_like(var))\n",
    "        if training:\n",
    "            self.variable_mean.assign(mean)\n",
    "            self.variable_var.assign(var)\n",
    "            self.ema.apply([self.variable_mean,self.variable_var])\n",
    "        else:\n",
    "            mean=self.ema.average(self.variable_mean)\n",
    "            var=self.ema.average(self.variable_var)\n",
    "        p=-tf.divide(x-mean,tf.sqrt(var+self.epsilon)) # 在InterestLayer中，结果为[batch_size, seq_len, emb_dim*3]\n",
    "        p=tf.divide(1,1+tf.pow(math.e,p)) # 在InterestLayer中，结果为[batch_size, seq_len, emb_dim*3]\n",
    "        return tf.multiply(p,x)+tf.multiply(tf.multiply(self.alpha,1-p),x)\n",
    "\n",
    "    # 错误写法\n",
    "    # def __call__(self,x,training=True, *args, **kwargs):\n",
    "    #\n",
    "    #     mean=tf.reduce_mean(x,axis=0,keepdims=True) # 根据batch计算平均，在在InterestLayer中，结果为[1, seq_len, emb_dim*3]\n",
    "    #     var=tf.math.reduce_variance(x,axis=0,keepdims=True) # 根据batch计算平均，在InterestLayer中，结果为[1, seq_len, emb_dim*3]\n",
    "    #\n",
    "    #     print(\"initialize variable_mean and variable_var\")\n",
    "    #     variable_mean=tf.Variable(mean)\n",
    "    #     variable_var=tf.Variable(var)\n",
    "    #     if training:\n",
    "    #         self.ema.apply([variable_mean,variable_var])\n",
    "    #     else:\n",
    "    #         mean=self.ema.average(variable_mean) # 这时会返回None\n",
    "    #         var=self.ema.average(variable_var)\n",
    "    #     p=-tf.divide(x-mean,tf.sqrt(var+self.epsilon)) # 在InterestLayer中，结果为[batch_size, seq_len, emb_dim*3]\n",
    "    #     p=tf.divide(1,1+tf.pow(math.e,p)) # 在InterestLayer中，结果为[batch_size, seq_len, emb_dim*3]\n",
    "    #     return tf.multiply(p,x)+tf.multiply(tf.multiply(self.alpha,1-p),x)\n",
    "\n",
    "class InterestLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,attention_cells,units_list,*args,**kwargs):\n",
    "        super(InterestLayer,self).__init__(*args,**kwargs)\n",
    "        self.dense_layers=[tf.keras.layers.Dense(units=units,activation=None) for units in units_list]\n",
    "        self.attention_cells=attention_cells\n",
    "        self.scoring_layer=tf.keras.layers.Dense(units=1,activation=None)\n",
    "\n",
    "    def call(self, inputs, training=True, **kwargs):\n",
    "        # user_behaviors: [batch_size, seq_len, ad_field_num]\n",
    "        # user_behaviors_emb: [batch_size, seq_len, emb_dim]\n",
    "        # ad_emb: [batch_size, emb_dim]\n",
    "        user_behaviors,user_behaviors_emb,ad_emb=inputs\n",
    "\n",
    "        # calc attention scores\n",
    "        # seq_len=user_behaviors.shape[1]\n",
    "        # emb_dim=ad_emb.shape[-1]\n",
    "        # ad_emb=tf.reshape(tf.tile(ad_emb,multiples=[1,seq_len]),shape=[-1,seq_len,emb_dim]) #[batch_size, seq_len, emb_dim]\n",
    "        ad_emb=tf.broadcast_to(tf.expand_dims(ad_emb,axis=1),shape=user_behaviors_emb.shape) #[batch_size, seq_len, emb_dim]\n",
    "        out_product=tf.multiply(user_behaviors_emb,ad_emb) #[batch_size, seq_len, emb_dim]\n",
    "\n",
    "        dense_inputs=tf.concat([user_behaviors_emb,out_product,ad_emb],axis=-1) # [batch_size, seq_len, emb_dim*3]\n",
    "        for dense_layer,attention_cell in zip(self.dense_layers,self.attention_cells):\n",
    "            dense_inputs=dense_layer(dense_inputs)\n",
    "            dense_inputs=attention_cell(dense_inputs,training=training)\n",
    "\n",
    "        # dense_inputs: [batch_size, seq_len, units]\n",
    "        attention_scores=self.scoring_layer(dense_inputs) # [batch_size, seq_len, 1]\n",
    "        interest_emb=tf.multiply(attention_scores,user_behaviors_emb) # [batch_size, seq_len, emb_dim]\n",
    "\n",
    "        # mask\n",
    "        mask=tf.expand_dims(tf.where(tf.not_equal(user_behaviors[:,:,0],0),x=1.,y=0.),axis=-1) # [batch_size, seq_len, 1]\n",
    "        interest_emb=tf.multiply(interest_emb,mask)\n",
    "        interest_emb=tf.reduce_sum(interest_emb,axis=1) # [batch_size, emb_dim]\n",
    "        return interest_emb\n",
    "\n",
    "\n",
    "inputs=(user_behaviors,user_behaviors_emb,ad_emb)\n",
    "attention_cells=[PRelu()]\n",
    "units_list=[36]\n",
    "interest_layer=InterestLayer(attention_cells=attention_cells,units_list=units_list)\n",
    "interest_emb = interest_layer(inputs)\n",
    "interest_emb = interest_layer(inputs,training=False)\n",
    "print(interest_emb.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize variable_mean and variable_var\n",
      "initialize variable_mean and variable_var\n",
      "initialize variable_mean and variable_var\n",
      "(10, 2)\n"
     ]
    }
   ],
   "source": [
    "class DIN(tf.keras.Model):\n",
    "    def __init__(self,user_feat_num,user_field_num,\n",
    "                 ad_feat_num,ad_field_num,\n",
    "                 context_feat_num,context_field_num,emb_dim,\n",
    "                 dense_units_list,dense_cells,\n",
    "                 attention_units_list,attention_cells,\n",
    "                 *args,**kwargs):\n",
    "        super(DIN,self).__init__(*args,**kwargs)\n",
    "        self.emb_layer=EmbeddingLayer(user_feat_num=user_feat_num,user_field_num=user_field_num,\n",
    "                                      ad_feat_num=ad_feat_num,ad_field_num=ad_field_num,\n",
    "                                      context_feat_num=context_feat_num,context_field_num=context_field_num,\n",
    "                                      emb_dim=emb_dim)\n",
    "        self.interest_layer=InterestLayer(units_list=attention_units_list,attention_cells=attention_cells)\n",
    "        self.dense_layers=[tf.keras.layers.Dense(units=units,activation=None) for units in dense_units_list]\n",
    "        self.dense_cells=dense_cells\n",
    "        self.scoring_layer=tf.keras.layers.Dense(units=2,activation=None)\n",
    "\n",
    "    def call(self, inputs, training=True, mask=None):\n",
    "        _,user_behaviors,_,_=inputs\n",
    "        user_emb,user_behaviors_emb,ad_emb,context_emb=self.emb_layer(inputs)\n",
    "        interest_emb=self.interest_layer((user_behaviors,user_behaviors_emb,ad_emb),training=training)\n",
    "\n",
    "        dense_inputs=tf.concat([user_emb,interest_emb,context_emb],axis=1)\n",
    "        for dense_layer,dense_cell in zip(self.dense_layers,self.dense_cells):\n",
    "            dense_inputs=dense_layer(dense_inputs)\n",
    "            dense_inputs=dense_cell(dense_inputs)\n",
    "        output=self.scoring_layer(dense_inputs)\n",
    "\n",
    "        return output\n",
    "\n",
    "user_feat_num=3\n",
    "user_field_num=3\n",
    "ad_feat_num=4\n",
    "ad_field_num=2\n",
    "context_feat_num=5\n",
    "context_field_num=3\n",
    "emb_dim=6\n",
    "batch_size=10\n",
    "seq_len=7\n",
    "user_feat_batch=np.random.randint(1,1+user_feat_num,size=[batch_size,user_field_num]).astype(np.int32)\n",
    "\n",
    "user_behaviors=[]\n",
    "for _ in range(batch_size):\n",
    "    seq=np.random.choice(range(1,1+ad_feat_num),size=[np.random.randint(1,1+seq_len),2]).astype(np.int32).tolist()\n",
    "    user_behaviors.append(seq)\n",
    "user_behaviors=tf.keras.preprocessing.sequence.pad_sequences(user_behaviors,maxlen=seq_len,padding=\"post\")\n",
    "\n",
    "ad_feat_batch=np.random.randint(1,1+ad_feat_num,size=[batch_size,ad_field_num]).astype(np.int32)\n",
    "context_feat_batch=np.random.randint(1,1+context_feat_num,size=[batch_size,context_field_num]).astype(np.int32)\n",
    "\n",
    "din=DIN(user_feat_num=user_feat_num,\n",
    "user_field_num=user_field_num,\n",
    "ad_feat_num=ad_feat_num,\n",
    "ad_field_num=ad_field_num,\n",
    "context_feat_num=context_feat_num,\n",
    "context_field_num=context_field_num,emb_dim=emb_dim,\n",
    "        attention_units_list=[36],attention_cells=[Dice()],\n",
    "        dense_units_list=[200,80],dense_cells=[Dice(),Dice()])\n",
    "\n",
    "inputs=(user_feat_batch,user_behaviors,ad_feat_batch,context_feat_batch)\n",
    "output=din(inputs)\n",
    "output=din(inputs,training=False)\n",
    "print(output.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}