{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(5, 4), dtype=float32, numpy=\narray([[ 1.4819505 ,  4.0658646 ,  0.9037664 ,  0.2711991 ],\n       [ 3.3790822 ,  2.7338052 , -0.6880965 , -1.9817553 ],\n       [ 2.947948  ,  3.555347  , -0.9837462 , -0.95452225],\n       [ 3.223913  ,  8.262224  , -1.4352503 , -0.7414987 ],\n       [ 1.8996357 ,  6.6446342 , -2.805888  , -2.136921  ]],\n      dtype=float32)>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class InnerProductLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    该layer为根据embedding矩阵的输入计算paper中的l_p向量\n",
    "    \"\"\"\n",
    "    def __init__(self,n,field_num,*args,**kwargs):\n",
    "        \"\"\"\n",
    "\n",
    "        :param n: int 指L_p的维度\n",
    "        :param field_num: int 指特征中有多少个field，例如如果输入有性别类型、年龄段，那么field就是2\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "        super(InnerProductLayer,self).__init__(*args,**kwargs)\n",
    "        self.n=n\n",
    "        self.field_num=field_num\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W=tf.Variable(tf.random.truncated_normal(shape=[1,self.n,self.field_num,self.field_num]))\n",
    "\n",
    "    def call(self, z,**kwargs):\n",
    "        \"\"\"\n",
    "\n",
    "        :param z: tensor shape:[batch_size,field_num,emb_dim]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        z=tf.expand_dims(z,axis=1) # [batch_size,1,field_num,emb_dim]\n",
    "        transpose_z=tf.transpose(z,perm=[0,1,3,2])  # [batch_size,1,emb_dim,field_num]\n",
    "\n",
    "        P=tf.matmul(z,transpose_z) # [batch_size,1,field_num,field_num] 对每个样本来说，他的嵌入向量矩阵为[field_num,emb_dim]，对其求内积\n",
    "        L_P=self.W*P # [batch_size,n,field_num,field_num] 然后做线性变换\n",
    "        L_P=tf.reduce_sum(L_P,axis=[2,3]) # [batch_size,n]\n",
    "        return L_P\n",
    "\n",
    "inputs=np.random.random(size=[5,6,3]).astype(np.float32)\n",
    "inner_layer=InnerProductLayer(4,field_num=6)\n",
    "\n",
    "inner_layer(inputs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(5, 4), dtype=float32, numpy=\narray([[-100.68285 ,  163.0145  ,  -41.71023 ,  175.52466 ],\n       [ -54.980583,   89.018456,  -22.776995,   95.849976],\n       [ -67.933876,  109.99099 ,  -28.143194,  118.43199 ],\n       [ -73.81021 ,  119.505295,  -30.577604,  128.67645 ],\n       [-100.447876,  162.63405 ,  -41.612896,  175.115   ]],\n      dtype=float32)>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class OuterProductLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,n,emb_dim,*args,**kwargs):\n",
    "        super(OuterProductLayer,self).__init__(*args,**kwargs)\n",
    "        self.n=n\n",
    "        self.emb_dim=emb_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W=tf.Variable(tf.random.truncated_normal(shape=[1,self.n,self.emb_dim,self.emb_dim]))\n",
    "\n",
    "    def call(self, z, **kwargs):\n",
    "        \"\"\"\n",
    "\n",
    "        :param z: tensor shape:[batch_size,field_num,emb_dim]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 计算方法就是paper中说的，没啥好谈的\n",
    "        f_sum=tf.expand_dims(tf.reduce_sum(z,axis=1),axis=2) # [batch_size,emb_dim,1]\n",
    "        transpose_f_sum=tf.transpose(f_sum,perm=[0,2,1])  # [batch_size,emb_dim,1]\n",
    "        P=tf.expand_dims(tf.matmul(transpose_f_sum,f_sum),axis=1) # [batch_size,1,emb_dim,emb_dim]\n",
    "        L_P=self.W*P # [batch_size,n,emb_dim,emb_dim]\n",
    "        L_P=tf.reduce_sum(L_P,axis=[2,3])\n",
    "        return L_P\n",
    "\n",
    "inputs=np.random.random(size=[5,6,3]).astype(np.float32)\n",
    "outer_layer=OuterProductLayer(4,emb_dim=3)\n",
    "\n",
    "outer_layer(inputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(5, 4), dtype=float32, numpy=\narray([[ 4.2083025 , -0.8672843 ,  0.22887194,  1.0576658 ],\n       [ 3.4540327 , -0.46194398,  1.4278572 , -1.6943581 ],\n       [ 1.8267586 , -2.5708835 , -2.0168152 , -0.49889088],\n       [ 2.3245084 , -2.7218816 ,  0.12577295, -1.5272183 ],\n       [ 1.6866165 , -2.4964826 , -3.2161415 , -0.2379616 ]],\n      dtype=float32)>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LzLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,n,field_num,emb_dim,*args,**kwargs):\n",
    "        super(LzLayer,self).__init__(*args,**kwargs)\n",
    "        self.n=n\n",
    "        self.field_num=field_num\n",
    "        self.emb_dim=emb_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W=tf.Variable(tf.random.truncated_normal(shape=[1,self.n,self.field_num,self.emb_dim]))\n",
    "\n",
    "    def call(self, z, **kwargs):\n",
    "        \"\"\"\n",
    "\n",
    "        :param z: tensor shape:[batch_size,field_num,emb_dim]\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        z=tf.expand_dims(z,axis=1) # [batch_size,1,field_num,emb_dim]\n",
    "        L_Z=z*self.W # [batch_size,n,field_num,emb_dim]\n",
    "        L_Z=tf.reduce_sum(L_Z,axis=[2,3]) # [batch_size,n]\n",
    "\n",
    "        return L_Z\n",
    "inputs=np.random.random(size=[5,6,3]).astype(np.float32)\n",
    "lz_layer=LzLayer(4,field_num=6,emb_dim=3)\n",
    "\n",
    "lz_layer(inputs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(5, 2), dtype=float32, numpy=\narray([[0.4918684 , 0.49843407],\n       [0.50047046, 0.49994487],\n       [0.5006351 , 0.49992555],\n       [0.49324763, 0.49785638],\n       [0.49482873, 0.4987411 ]], dtype=float32)>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ProductNeuralNetworksModel(tf.keras.Model):\n",
    "    def __init__(self,input_dims,emb_dim,dense_units,d1,product_type=\"inner\",scoring_layer_units=2,*args,**kwargs):\n",
    "        \"\"\"\n",
    "\n",
    "        :param input_dims: [int] 各个field的embedding layer的input_dim，代表某个field的特征有多少个不同的取值\n",
    "        :param emb_dim: int 嵌入向量的维度\n",
    "        :param dense_units:\n",
    "        :param d1: int paper中的l1层的输入维度，指的是paper中的D_1\n",
    "        :param product_type: str inner or outer\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "        super(ProductNeuralNetworksModel,self).__init__(*args,**kwargs)\n",
    "        self.emb_layers=list()\n",
    "        self.field_num=len(input_dims)\n",
    "        self.emb_dim=emb_dim\n",
    "        for input_dim in input_dims:\n",
    "            self.emb_layers.append(tf.keras.layers.Embedding(input_dim=input_dim,output_dim=emb_dim))\n",
    "        self.lz_layer=LzLayer(n=d1,field_num=self.field_num,emb_dim=self.emb_dim)\n",
    "        if product_type==\"inner\":\n",
    "            self.product_layer=InnerProductLayer(n=d1,field_num=self.field_num)\n",
    "        elif product_type==\"outer\":\n",
    "            self.product_layer=OuterProductLayer(n=d1,emb_dim=self.emb_dim)\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        self.b1=tf.Variable(tf.zeros(shape=[d1]))\n",
    "        self.dense_layer=tf.keras.layers.Dense(dense_units)\n",
    "        self.scoring_layer=tf.keras.layers.Dense(scoring_layer_units,activation=tf.nn.sigmoid)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param inputs: tensor ,shape :[batch_size, len(self.input_dims)]\n",
    "        :param training:\n",
    "        :param mask:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        emb_vecs=list()\n",
    "        for i,emb_layer in enumerate(self.emb_layers):\n",
    "            emb_vecs.append(emb_layer(inputs[:,i]))\n",
    "\n",
    "        # emb_vecs里每一个元素都是(batch_size, emb_dim)\n",
    "        z=tf.stack(emb_vecs,axis=1) # shape :[batch_size, field_num, emb_dim]\n",
    "        l_z=self.lz_layer(z)\n",
    "        l_p=self.product_layer(z)\n",
    "\n",
    "        l1=l_z+l_p+self.b1\n",
    "        l1=tf.nn.relu(l1)\n",
    "        l2=self.dense_layer(l1)\n",
    "        y=self.scoring_layer(l2)\n",
    "        return y\n",
    "\n",
    "input_arr=np.random.randint(0,5,size=[5,3])\n",
    "inner_model=ProductNeuralNetworksModel([5,5,5],emb_dim=2,dense_units=2,d1=3,product_type=\"inner\")\n",
    "inner_model(input_arr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}