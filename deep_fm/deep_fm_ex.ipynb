{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 在原版paper的deepfm算法做了一点改进，即fm_component和deep component输出的都不只是一个数了，而是一个响亮\n",
    "# 大量参照https://github.com/ChenglongChen/tensorflow-DeepFM\n",
    "# 他对deep fm进行了一点点优化，虽然数据描述不太明白，但是其数据结构设计得确实好，很精简也非常适合这个算法\n",
    "# deep fm只能处理one-hot类型的，无法直接处理multi-hot特征\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat_indices_arr\n",
      "[[2. 9. 3.]\n",
      " [2. 1. 3.]\n",
      " [9. 4. 6.]\n",
      " [7. 5. 4.]\n",
      " [3. 1. 8.]\n",
      " [8. 9. 1.]\n",
      " [1. 0. 2.]\n",
      " [4. 9. 5.]\n",
      " [4. 0. 9.]]\n",
      "\n",
      "feat_vals_arr\n",
      "[[1.         1.         0.9578554 ]\n",
      " [1.         1.         0.9417463 ]\n",
      " [1.         1.         0.65312517]\n",
      " [1.         1.         0.7425048 ]\n",
      " [1.         1.         0.6467193 ]\n",
      " [1.         1.         0.4838316 ]\n",
      " [1.         1.         0.01414096]\n",
      " [1.         1.         0.57208407]\n",
      " [1.         1.         0.06287198]]\n",
      "\n",
      "emb vectors\n",
      "tf.Tensor(\n",
      "[[[-0.00250721  0.03223243 -0.02837677  0.02888547]\n",
      "  [-0.03880321  0.03691218  0.02859639  0.0448879 ]\n",
      "  [-0.03320575  0.02791285 -0.03738778 -0.03431834]]\n",
      "\n",
      " [[-0.00250721  0.03223243 -0.02837677  0.02888547]\n",
      "  [-0.00303785  0.04685927  0.03928286 -0.03207158]\n",
      "  [-0.0326473   0.02744341 -0.036759   -0.03374117]]\n",
      "\n",
      " [[-0.03880321  0.03691218  0.02859639  0.0448879 ]\n",
      "  [-0.04194355  0.03691251 -0.02264855 -0.02871122]\n",
      "  [ 0.00264363  0.03192767  0.00278697  0.02896227]]\n",
      "\n",
      " [[ 0.0323647   0.02443125  0.04490982  0.0029154 ]\n",
      "  [-0.04850848 -0.00497992  0.0099975  -0.00215516]\n",
      "  [-0.03114329  0.02740772 -0.01681666 -0.02131822]]\n",
      "\n",
      " [[-0.03466677  0.02914098 -0.03903281 -0.0358283 ]\n",
      "  [-0.00303785  0.04685927  0.03928286 -0.03207158]\n",
      "  [ 0.02394841  0.01873174  0.01584772 -0.00781589]]], shape=(5, 3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,feat_dim,field_num,emb_dim,*args,**kwargs):\n",
    "        super(EmbeddingLayer,self).__init__(*args,**kwargs)\n",
    "        self.feat_dim=feat_dim\n",
    "        self.field_num=field_num\n",
    "        self.emb_dim=emb_dim\n",
    "        self.emb_layer=tf.keras.layers.Embedding(input_dim=feat_dim,output_dim=emb_dim)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        :param inputs: (feat_indices_batch, feat_value_batch)，分为两部分，你可以把他看做一个手动构造的稀疏矩阵\n",
    "            例如，如果feat_indices_batch的数据为[[1,5,9],[2,7,8]]；feat_value_batch的数据为[[1,1,2.3],[1,1,0.98]]\n",
    "            假设第一个样本的特征向量是x，那么x[1]=1, x[5]=1, x[9]=2.3，其余位置取值均为0。\n",
    "            这样构造是因为每个样本都有field_num个field，每个field的取值只有一种（one-hot或者连续值）\n",
    "            也就是说每个样本都有field_num个不为0的特征维度。而deep fm算法的嵌入方法是对每一个field嵌入，不管是不是连续值都要嵌入，然后再乘以特征取值\n",
    "            例如x[9]=2.3，那就要从emb_table里找到第9个emb_vector，然后乘以2.3\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # feat_indices_batch: [batch_size, field_num]\n",
    "        # feat_value_batch: [batch_size, field_num]\n",
    "        feat_indices_batch,feat_value_batch=inputs\n",
    "        # 两者形状要相同，并且二者的第二个轴取值维度都是field_num个\n",
    "        assert feat_indices_batch.shape==feat_value_batch.shape\n",
    "        assert feat_indices_batch.shape[1:]==[self.field_num]\n",
    "\n",
    "        emb_vectors=self.emb_layer(feat_indices_batch) # [batch_size, field_num, emb_dim]\n",
    "        feat_value_batch = tf.expand_dims(feat_value_batch,axis=-1) # [batch_size, field_num, 1]\n",
    "\n",
    "        # broadcast性质 feat_value_batch会被看做[batch_size, field_num, emb_dim]\n",
    "        emb_vectors = tf.multiply(emb_vectors,feat_value_batch) # [batch_size, field_num, emb_dim]\n",
    "        return emb_vectors\n",
    "\n",
    "\n",
    "feat_indices_arr=[np.random.choice(range(10),size=[1,3],replace=False) for _ in range(9)]\n",
    "feat_indices_arr=np.concatenate(feat_indices_arr,axis=0).astype(np.float32)\n",
    "print(\"feat_indices_arr\")\n",
    "print(feat_indices_arr) #[10,3]\n",
    "\n",
    "feat_vals_arr=np.concatenate((np.ones(shape=[9,2]),\n",
    "                              np.random.random(size=[9,1])),axis=1).astype(np.float32)\n",
    "print(\"\\nfeat_vals_arr\")\n",
    "print(feat_vals_arr) # [10,3]\n",
    "\n",
    "input_ds=tf.data.Dataset.from_tensor_slices((feat_indices_arr,feat_vals_arr))\n",
    "batched_ds=input_ds.batch(5)\n",
    "iterator=iter(batched_ds)\n",
    "input_batch=next(iterator)\n",
    "\n",
    "\n",
    "print(\"\\nemb vectors\")\n",
    "emb_layer=EmbeddingLayer(feat_dim=10,field_num=3,emb_dim=4)\n",
    "emb_vectors=emb_layer(input_batch)\n",
    "print(emb_vectors)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "emb vectors\n",
      "tf.Tensor(\n",
      "[[[ 0.01574266  0.03361524 -0.02441003  0.03478476]\n",
      "  [ 0.03548969 -0.0261873   0.04189844 -0.00808696]\n",
      "  [ 0.00147651  0.01941362  0.01325359 -0.01578611]]\n",
      "\n",
      " [[ 0.01574266  0.03361524 -0.02441003  0.03478476]\n",
      "  [-0.0356837   0.01019305  0.0449813   0.02894637]\n",
      "  [-0.01997713  0.00182355 -0.0290976   0.00926884]]\n",
      "\n",
      " [[-0.02286857  0.00208749 -0.03330912  0.01061039]\n",
      "  [ 0.00792656 -0.04649333 -0.01925454 -0.03074418]\n",
      "  [ 0.00253944  0.03338931  0.02279472 -0.02715039]]\n",
      "\n",
      " [[ 0.00792656 -0.04649333 -0.01925454 -0.03074418]\n",
      "  [ 0.01095372 -0.01040577 -0.00403966 -0.0331562 ]\n",
      "  [ 0.01182     0.00274881  0.00316286  0.01265087]]\n",
      "\n",
      " [[ 0.00293436  0.03858181  0.02633962 -0.03137266]\n",
      "  [ 0.00792656 -0.04649333 -0.01925454 -0.03074418]\n",
      "  [-0.00686075  0.00195977  0.00864836  0.00556539]]], shape=(5, 3, 4), dtype=float32)\n",
      "\n",
      "fm_outputs\n",
      "tf.Tensor(\n",
      "[[ 0.17007205  0.30602217  1.2417642   0.85751283]\n",
      " [-2.3149729   1.5989451   2.3062763   0.10335954]\n",
      " [-1.5165918  -0.9349031  -0.09659993  3.3051565 ]\n",
      " [ 0.45575795 -0.16946109 -0.27839327  0.7849681 ]\n",
      " [-0.81821007 -0.9286904  -0.42791864  2.198755  ]], shape=(5, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class FMComponent(tf.keras.layers.Layer):\n",
    "    def __init__(self,feat_dim,field_num,emb_dim,*args,**kwargs):\n",
    "        super(FMComponent,self).__init__(*args,**kwargs)\n",
    "        self.feat_dim=feat_dim\n",
    "        self.field_num=field_num\n",
    "        self.emb_dim=emb_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w=tf.Variable(initial_value=tf.random.truncated_normal(shape=[self.feat_dim, self.emb_dim]))\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\"\n",
    "\n",
    "        :param inputs: (raw_input_batch,emb_vectors) ，其中raw_input_batch是feat_indices_batch,feat_value_batch 用于计算一阶term\n",
    "            后者是emb_layer的输出\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        raw_input_batch,emb_vectors=inputs # emb_vectors: [batch_size, field_num, emb_dim]\n",
    "        # feat_indices_batch: [batch_size, field_num]\n",
    "        # feat_value_batch: [batch_size, field_num]\n",
    "        feat_indices_batch,feat_value_batch=raw_input_batch\n",
    "\n",
    "        # first order term\n",
    "        # 使用feat_indices找到embedding_lookup快速找到field_num个权重然后做相乘\n",
    "        # 例如，如果一个样本x，他在特征维度1、3、5上有取值，那么他的feat_indices=[1,3,5]。那只需要从self.w找到第1、3、5个数就可以了\n",
    "        # 这样的计算方法更加快速\n",
    "        # 改进1 一阶输出也是一个向量而非一个标量，这就要求self.w的shape为[self.feat_dim, self.emb_dim]\n",
    "        weights=tf.nn.embedding_lookup(params=self.w,ids=tf.cast(feat_indices_batch,tf.int32)) # [batch_size, field_num, self.emb_dim]\n",
    "        # 需要对feat_value_batch扩充一下，不然无法进行broadcast\n",
    "        first_order_term = tf.multiply(tf.expand_dims(feat_value_batch,axis=2),weights) # [batch_size, field_num, emb_dim]\n",
    "        first_order_term = tf.reduce_sum(first_order_term,axis=1) # [batch_size, emb_dim]\n",
    "\n",
    "        # second order term\n",
    "        # 下面这个是fm算法的优化算法 和平方减去平方和\n",
    "        sum_square=tf.square(tf.reduce_sum(emb_vectors,axis=1)) # [batch_size, emb_dim]\n",
    "        square_sum=tf.reduce_sum(tf.square(emb_vectors),axis=1) # [batch_size, emb_dim]\n",
    "\n",
    "        second_order_term=1/2*tf.subtract(sum_square,square_sum) # [batch_size, emb_dim]\n",
    "        y_fm=first_order_term+second_order_term\n",
    "        return y_fm\n",
    "\n",
    "feat_indices_arr=[np.random.choice(range(10),size=[1,3],replace=False) for _ in range(9)]\n",
    "feat_indices_arr=np.concatenate(feat_indices_arr,axis=0).astype(np.float32)\n",
    "feat_vals_arr=np.concatenate((np.ones(shape=[9,2]),\n",
    "                              np.random.random(size=[9,1])),axis=1).astype(np.float32)\n",
    "\n",
    "input_ds=tf.data.Dataset.from_tensor_slices((feat_indices_arr,feat_vals_arr))\n",
    "batched_ds=input_ds.batch(5)\n",
    "iterator=iter(batched_ds)\n",
    "input_batch=next(iterator)\n",
    "\n",
    "\n",
    "print(\"\\nemb vectors\")\n",
    "emb_layer=EmbeddingLayer(feat_dim=10,field_num=3,emb_dim=4)\n",
    "emb_vectors=emb_layer(input_batch)\n",
    "print(emb_vectors)\n",
    "\n",
    "fm_component=FMComponent(feat_dim=10,field_num=3,emb_dim=4)\n",
    "fm_inputs=(input_batch,emb_vectors)\n",
    "fm_outputs=fm_component(fm_inputs)\n",
    "print(\"\\nfm_outputs\")\n",
    "print(fm_outputs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class DeepComponent(tf.keras.layers.Layer):\n",
    "    def __init__(self,deep_units_list,*args,**kwargs):\n",
    "        super(DeepComponent,self).__init__(*args,**kwargs)\n",
    "        self.deep_layers=list()\n",
    "        for deep_units in deep_units_list:\n",
    "            self.deep_layers.append(tf.keras.layers.Dense(units=deep_units,activation=tf.nn.relu))\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        for deep_layer in self.deep_layers:\n",
    "            inputs=deep_layer(inputs)\n",
    "        return inputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(5, 2), dtype=float32, numpy=\narray([[0.24492802, 0.755072  ],\n       [0.5246791 , 0.4753209 ],\n       [0.39354166, 0.6064583 ],\n       [0.5969774 , 0.40302256],\n       [0.2637145 , 0.73628557]], dtype=float32)>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class DeepFM(tf.keras.Model):\n",
    "    def __init__(self,feat_dim,field_num,emb_dim,deep_units_list,scoring_units=2,*args,**kwargs):\n",
    "        super(DeepFM,self).__init__(*args,**kwargs)\n",
    "\n",
    "        self.emb_layer=EmbeddingLayer(feat_dim=feat_dim,field_num=field_num,emb_dim=emb_dim)\n",
    "        self.fm_component=FMComponent(feat_dim=feat_dim,field_num=field_num,emb_dim=emb_dim)\n",
    "        self.deep_component=DeepComponent(deep_units_list=deep_units_list)\n",
    "        self.scoring_layer=tf.keras.layers.Dense(units=scoring_units,activation=None)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        emb_vectors=self.emb_layer(inputs)\n",
    "\n",
    "        fm_inputs=(inputs,emb_vectors)\n",
    "        y_fm=self.fm_component(fm_inputs)\n",
    "\n",
    "        deep_inputs=tf.reshape(emb_vectors,shape=[emb_vectors.shape[0],-1])\n",
    "        y_deep=self.deep_component(deep_inputs)\n",
    "        y = self.scoring_layer(tf.concat((y_fm,y_deep),axis=1))\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "feat_indices_arr=[np.random.choice(range(10),size=[1,3],replace=False) for _ in range(9)]\n",
    "feat_indices_arr=np.concatenate(feat_indices_arr,axis=0).astype(np.float32)\n",
    "feat_vals_arr=np.concatenate((np.ones(shape=[9,2]),\n",
    "                              np.random.random(size=[9,1])),axis=1).astype(np.float32)\n",
    "\n",
    "input_ds=tf.data.Dataset.from_tensor_slices((feat_indices_arr,feat_vals_arr))\n",
    "batched_ds=input_ds.batch(5)\n",
    "iterator=iter(batched_ds)\n",
    "input_batch=next(iterator)\n",
    "\n",
    "deep_fm_model=DeepFM(feat_dim=10,field_num=3,emb_dim=4,deep_units_list=[10,8])\n",
    "deep_fm_model(input_batch)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.67044261 0.75686147]\n",
      " [0.14368512 0.98200156]\n",
      " [0.7414385  0.31037816]]\n",
      "tf.Tensor(\n",
      "[[0.47840872 0.52159128]\n",
      " [0.30188948 0.69811052]\n",
      " [0.60612684 0.39387316]], shape=(3, 2), dtype=float64)\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}